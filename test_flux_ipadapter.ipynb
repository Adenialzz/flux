{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import CLIPVisionModelWithProjection, CLIPImageProcessor \n",
    "\n",
    "device = 'cuda'\n",
    "ae_path = '/home/jeeves/JJ_Projects/klara_models/flux_dev/train-model/ae.safetensors'\n",
    "transformer_path = '/home/jeeves/JJ_Projects/klara_models/flux_dev/train-model/flux1-dev.safetensors'\n",
    "os.environ['FLUX_DEV'] = transformer_path\n",
    "os.environ['AE'] = ae_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flux.util import load_ae, load_clip, load_flow_model, load_t5\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# load models\n",
    "transformer = load_flow_model(name='flux-dev', device=device, hf_download=False)\n",
    "\n",
    "print('**** flux attn processor before load ip-adapter *********')\n",
    "attn_procs = transformer.attn_processors\n",
    "for k, v in attn_procs.items():\n",
    "    print(k, v)\n",
    "\n",
    "transformer.load_ip_adapter(image_encoder_path='openai/clip-vit-large-patch14', ip_model_path='/home/jeeves/JJ_Projects/github/ComfyUI/models/xlabs/ipadapters/ip_adapter.safetensors')\n",
    "\n",
    "print('**** flux attn processor after load ip-adapter *********')\n",
    "attn_procs = transformer.attn_processors\n",
    "for k, v in attn_procs.items():\n",
    "    print(k, v)\n",
    "\n",
    "\n",
    "ae = load_ae(name='flux-dev', device=device, hf_download=False)\n",
    "clip = load_clip(device=device)\n",
    "t5 = load_t5(device=device, max_length=512)\n",
    "\n",
    "class ImageProjector:\n",
    "\n",
    "    def __init__(self):\n",
    "        image_encoder_path = 'openai/clip-vit-large-patch14'\n",
    "        self.image_encoder = CLIPVisionModelWithProjection.from_pretrained(image_encoder_path).to(\n",
    "            'cuda', dtype=torch.float16\n",
    "        )\n",
    "        self.clip_image_processor = CLIPImageProcessor()\n",
    "\n",
    "    def __call__(self, image_prompt: Image.Image | np.ndarray):\n",
    "        # encode image-prompt embeds\n",
    "        image_prompt = self.clip_image_processor(\n",
    "            images=image_prompt,\n",
    "            return_tensors=\"pt\"\n",
    "        ).pixel_values\n",
    "\n",
    "        image_prompt = image_prompt.to(self.image_encoder.device)\n",
    "        image_prompt_embeds = self.image_encoder(image_prompt).image_embeds.to( device='cuda', dtype=torch.bfloat16)\n",
    "        return image_prompt_embeds\n",
    "\n",
    "img_embbeder = ImageProjector()\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import einops\n",
    "# 现在我们需要一步步自己实现下面的采样函数\n",
    "from flux.sampling import get_noise, prepare, denoise, get_schedule, unpack\n",
    "from songmisc.utils import plot_multi_images\n",
    "\n",
    "\n",
    "width, height = 1024, 1024\n",
    "device = torch.device('cuda')\n",
    "dtype = torch.bfloat16\n",
    "seed = 42\n",
    "# prompt = \"a handsome man wearing suit, running on the street\"\n",
    "# prompt = \"a handsome man wearing suit, standing in the classroom, hugging with a woman with white dress\"\n",
    "# prompt = 'anime style, a man wearing suit, running on the street'\n",
    "prompt = 'anime style, (a man wearing black white interleaved suit and glasses) standing on the street, hugging with (a woman with ponytail hair, wearing white t-shirt and jeans), looking at each other'\n",
    "\n",
    "steps = 50\n",
    "guidance = 3.5\n",
    "ip_scale = 0.5\n",
    "\n",
    "path = 'imgs/qye.png'\n",
    "ref_image = Image.open(path)\n",
    "image_emb = img_embbeder(ref_image)\n",
    "print('image emb', image_emb.shape)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    # 获取噪声\n",
    "    x_T = get_noise(num_samples=1, height=height, width=width, device=device, dtype=dtype, seed=seed)\n",
    "\n",
    "    # 准备输入：img，img_ids，txt，txt_ids, vec\n",
    "    inputs_dict = prepare(t5, clip, x_T, prompt)\n",
    "    timesteps = get_schedule(steps, inputs_dict[\"img\"].shape[1], shift=True)\n",
    "\n",
    "    # 去噪循环\n",
    "    img = inputs_dict['img']\n",
    "    guidance_vec = torch.full((img.shape[0], ), guidance, device=device, dtype=img.dtype)\n",
    "    for t_curr, t_prev in zip(timesteps[: -1], timesteps[1: ]):  \n",
    "        t_vec = torch.full((img.shape[0], ), t_curr, device=device, dtype=img.dtype)\n",
    "        pred = transformer(\n",
    "            img=img, img_ids=inputs_dict['img_ids'],\n",
    "            txt=inputs_dict['txt'], txt_ids=inputs_dict['txt_ids'],\n",
    "            y = inputs_dict['vec'],\n",
    "            timesteps=t_vec,\n",
    "            guidance=guidance_vec,\n",
    "            image_embeddings=image_emb,\n",
    "            ip_scale=ip_scale\n",
    "        )\n",
    "\n",
    "        img = img + (t_prev - t_curr) * pred \n",
    "    x = unpack(img.float(), height, width)\n",
    "\n",
    "    with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "        x = ae.decode(x)\n",
    "\n",
    "    x = x.clamp(-1, 1)\n",
    "    x = einops.rearrange(x[0], \"c h w -> h w c\")\n",
    "    img = Image.fromarray((127.5 * (x + 1.0)).cpu().byte().numpy())\n",
    "\n",
    "plot_multi_images([ref_image, img])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
