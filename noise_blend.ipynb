{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# set params\n",
    "ae_path = '/home/jeeves/JJ_Projects/klara_models/flux_dev/train-model/ae.safetensors'\n",
    "transformer_path = '/home/jeeves/JJ_Projects/klara_models/flux_dev/train-model/flux1-dev.safetensors'\n",
    "os.environ['FLUX_DEV'] = transformer_path\n",
    "os.environ['AE'] = ae_path\n",
    "\n",
    "width, height = 1024, 1024\n",
    "device = torch.device('cuda')\n",
    "dtype = torch.bfloat16\n",
    "seed = 42\n",
    "prompt = \"a cute boy running on the street\"\n",
    "\n",
    "guidance = 3.5\n",
    "steps = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flux.util import load_ae, load_clip, load_flow_model, load_t5\n",
    "\n",
    "# load models\n",
    "ae = load_ae(name='flux-dev', device=device, hf_download=False)\n",
    "clip = load_clip(device=device)\n",
    "t5 = load_t5(device=device, max_length=512)\n",
    "transformer = load_flow_model(name='flux-dev', device=device, hf_download=False)\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from flux.modules.autoencoder import AutoEncoder\n",
    "from flux.sampling import prepare, get_schedule, denoise, unpack\n",
    "from torchvision import transforms\n",
    "import einops\n",
    "import numpy as np\n",
    "\n",
    "@torch.inference_mode()\n",
    "def encode(img: Image.Image, ae: AutoEncoder, device=torch.device('cuda')):\n",
    "    img_np = np.array(img)\n",
    "    img_tensor = (torch.from_numpy(img_np).permute(2, 0, 1).float() / 127.5 - 1).unsqueeze(0)\n",
    "    img_tensor = img_tensor.to(device=device)\n",
    "    latent = ae.encode(img_tensor)\n",
    "    return latent.to(dtype=torch.bfloat16)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def decode(latent: torch.Tensor, ae: AutoEncoder):\n",
    "    img_tensor = ae.decode(latent)\n",
    "    img_tensor = einops.rearrange(img_tensor[0], \"c h w -> h w c\")\n",
    "    img_tensor = img_tensor.clamp(-1, 1)\n",
    "    img = Image.fromarray((127.5 * (img_tensor + 1.0)).cpu().byte().numpy())\n",
    "    return img\n",
    "\n",
    "\n",
    "steps = 4\n",
    "with torch.inference_mode():\n",
    "    input_image = Image.open('test.jpg')\n",
    "\n",
    "    latent = encode(input_image, ae)\n",
    "    print(latent.shape)\n",
    "\n",
    "    inputs_dict = prepare(t5, clip, latent, prompt=\"catoon style, a man hugging with a woman\")\n",
    "\n",
    "    img = inputs_dict['img']\n",
    "    img_ids = inputs_dict['img_ids']\n",
    "    txt = inputs_dict['txt']\n",
    "    txt_ids = inputs_dict['txt_ids']\n",
    "    vec = inputs_dict['vec']\n",
    "\n",
    "    timesteps = get_schedule(num_steps=steps, image_seq_len=latent.shape[1])\n",
    "    inverse_timesteps = timesteps[:: -1]\n",
    "    print(timesteps)\n",
    "    print(inverse_timesteps)\n",
    "\n",
    "    # gamma = 0.0\n",
    "    # target_noise = torch.randn(img.shape, device=img.device, dtype=torch.float32)\n",
    "    # target_noise = torch.randn(img.shape, device=img.device, dtype=torch.bfloat16)\n",
    "\n",
    "    guidance = 7\n",
    "\n",
    "    use_rf_solver = False\n",
    "    guidance_vec = torch.full((img.shape[0], ), guidance, device=device, dtype=img.dtype)\n",
    "    for t_curr, t_prev in zip(inverse_timesteps[: -1], inverse_timesteps[1: ]):\n",
    "        t_vec = torch.full((img.shape[0], ), t_curr, device=device, dtype=img.dtype)\n",
    "        pred = transformer(\n",
    "            img = img, img_ids = img_ids,\n",
    "            txt = txt, txt_ids = txt_ids,\n",
    "            y = vec,\n",
    "            timesteps=t_vec,\n",
    "            guidance=guidance_vec\n",
    "        )\n",
    "\n",
    "        img = img + (t_prev - t_curr) * pred\n",
    "\n",
    "    res = denoise(transformer, img=img, img_ids=img_ids, txt=txt, txt_ids=txt_ids, vec=vec, timesteps=timesteps, guidance=guidance)\n",
    "    with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "        rect_img = decode(unpack(res, height, width), ae)\n",
    "        latent_img = decode(unpack(img, height, width), ae)\n",
    "        vae_rect_img = decode(latent, ae)\n",
    "\n",
    "    vae_rect_img.save('vae_rect.png')\n",
    "    rect_img.save('rect.png')\n",
    "    latent_img.save('latent.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import math\n",
    "import einops\n",
    "from typing import Callable\n",
    "from torchvision import transforms\n",
    "# 现在我们需要一步步自己实现下面的采样函数\n",
    "# from flux.sampling import get_noise, prepare, denoise, get_schedule, unpack\n",
    "\n",
    "def get_indices(mask_path: str):\n",
    "    mask_image = Image.open(mask_path)\n",
    "    vae_scale_factor = 8\n",
    "    patch_size = 2\n",
    "\n",
    "    width, height = mask_image.size\n",
    "\n",
    "    man_mask_resized = mask_image.resize((width // (vae_scale_factor * patch_size), height // (vae_scale_factor * patch_size)))\n",
    "\n",
    "    print(man_mask_resized.size)\n",
    "    man_mask_resized_tensor = transforms.ToTensor()(man_mask_resized)\n",
    "\n",
    "    print(man_mask_resized_tensor.shape)\n",
    "\n",
    "    index = torch.nn.Flatten()(man_mask_resized_tensor).squeeze()\n",
    "\n",
    "    # 将一维索引扩展到与A和B相同的形状\n",
    "    index = index.view(1, -1, 1).to(dtype=torch.int64)  # 现在index的形状是[1, 4096, 1]\n",
    "    return index\n",
    "\n",
    "def time_shift(mu: float, sigma: float, t: torch.Tensor):\n",
    "    return math.exp(mu) / (math.exp(mu) + (1 / t - 1) ** sigma)\n",
    "\n",
    "def get_lin_function(\n",
    "    x1: float = 256, y1: float = 0.5, x2: float = 4096, y2: float = 1.15\n",
    ") -> Callable[[float], float]:\n",
    "    m = (y2 - y1) / (x2 - x1)\n",
    "    b = y1 - m * x1\n",
    "    return lambda x: m * x + b\n",
    "\n",
    "steps = 4\n",
    "\n",
    "with torch.inference_mode():\n",
    "    # 获取噪声\n",
    "    # x_T = get_noise(num_samples=1, height=height, width=width, device=device, dtype=dtype, seed=seed)\n",
    "    x_T = torch.randn(\n",
    "        1,\n",
    "        16,  # channels\n",
    "        # 需要能够被 16 整除 ？ TODO  这里的 2 是为了下面能被 块的尺寸 pw/ph 整除？\n",
    "        2 * math.ceil(height / 16),  \n",
    "        2 * math.ceil(width / 16),\n",
    "        device=device,\n",
    "        dtype=dtype,\n",
    "        generator=torch.Generator(device=device).manual_seed(seed),\n",
    "    )\n",
    "    print(\"x_T shape:\", x_T.shape)  # [1, 16, 128, 128]\n",
    "\n",
    "    # 准备输入：img，img_ids，txt，txt_ids\n",
    "    # inputs_dict = prepare(t5, clip, x_T, prompt)\n",
    "    bs, c, h, w = x_T.shape\n",
    "\n",
    "    # 图块化  [1, 16, 128(64 * 2), 128] -> [1, 4096(64 * 64), 64(16 * 2 * 2)]   (bs, img_token_seq_len, dim_in)\n",
    "    # ph/pw 块高、宽, 都取 2。\n",
    "    # 这里 patch 的宽高都是写死 2 的话，是不是图块化/反图块化写在模型内部比较合理？\n",
    "    x_T_patchified = einops.rearrange(x_T, \"b c (h ph) (w pw) -> b (h w) (c ph pw)\", ph=2, pw=2)\n",
    "    img = x_T_patchified\n",
    "\n",
    "    # img_ids TODO   这是干嘛？？？？？？？？？\n",
    "    img_ids = torch.zeros(h // 2, w // 2, 3)\n",
    "    img_ids[..., 1] = img_ids[..., 1] + torch.arange(h // 2)[:, None]\n",
    "    img_ids[..., 2] = img_ids[..., 2] + torch.arange(w // 2)[None, :]\n",
    "    img_ids = einops.repeat(img_ids, \"h w c -> b (h w) c\", b=bs)\n",
    "    img_ids = img_ids.to(device)\n",
    "    print('img_ids shape:', img_ids.shape)  # [bs, img_token_seq_len, 3] [1, 4096, 3]\n",
    "\n",
    "    # 文本条件嵌入\n",
    "    txt = t5(prompt)\n",
    "    print('txt shape:', txt.shape)  # [bs, t5_seq_len, t5_dim]  [1, 512, 4096]\n",
    "\n",
    "    # txt_ids\n",
    "    txt_ids = torch.zeros(bs, txt.shape[1], 3)\n",
    "    txt_ids = txt_ids.to(device)\n",
    "    print('txt_ids shape:', txt_ids.shape)  # [bs, t5_seq_len, 3]   [1, 512, 3]\n",
    "\n",
    "    vec = clip(prompt)\n",
    "    print('clip vec shape:', vec.shape)  # [bs, clip_dim]    [1, 768]\n",
    "\n",
    "    # 获取采样时间步\n",
    "    # timesteps = get_schedule(steps, inputs_dict[\"img\"].shape[1], shift=True)\n",
    "    timesteps = torch.linspace(1, 0, steps+1)\n",
    "    shift = True\n",
    "    if shift:\n",
    "        base_shift = 0.5\n",
    "        max_shift = 1.15\n",
    "        image_seq_len = img.shape[1]\n",
    "        mu = get_lin_function(y1=base_shift, y2=max_shift)(image_seq_len)\n",
    "        timesteps = time_shift(mu, 1.0, timesteps)\n",
    "\n",
    "    timesteps = timesteps.tolist()\n",
    "\n",
    "    print('timesteps')\n",
    "    print(timesteps)\n",
    "\n",
    "    # 去噪循环\n",
    "    # x = denoise(transformer, **inputs_dict, guidance=guidance_scale, timesteps=timesteps)\n",
    "\n",
    "    # cfg 蒸馏的结果，直接将 guidance 标量值输入模型。只有 dev 模型有 guidance，schnell 模型没有,\n",
    "    guidance_vec = torch.full((img.shape[0], ), guidance, device=device, dtype=img.dtype)\n",
    "    for t_curr, t_prev in zip(timesteps[: -1], timesteps[1: ]):  # TODO 为啥这样写，和 RF 采样公式有关？？？\n",
    "        t_vec = torch.full((img.shape[0], ), t_curr, device=device, dtype=img.dtype)\n",
    "        pred = transformer(\n",
    "            img=img, img_ids=img_ids,\n",
    "            txt=txt, txt_ids=txt_ids,\n",
    "            y=vec,\n",
    "            timesteps=t_vec,\n",
    "            guidance=guidance_vec\n",
    "        )\n",
    "\n",
    "        img = img + (t_prev - t_curr) * pred   # RF 的采样公式 ？？？？？？？？\n",
    "    x = img\n",
    "\n",
    "    print(\"x_0 shape:\", x.shape)\n",
    "    rand_x = torch.randn(*x.shape).to(device=device)\n",
    "    print('rand x shape:', rand_x.shape)\n",
    "\n",
    "    index = get_indices('mask_man.png').to(device=device)\n",
    "    x = x * (index == 0) + rand_x * (index == 1)\n",
    "\n",
    "    # 反图块化\n",
    "    # x = unpack(x.float(), height, width)\n",
    "    x = einops.rearrange(x, \"b (h w) (c ph pw) -> b c (h ph) (w pw)\", h=64, w=64, ph=2, pw=2)\n",
    "    print(\"x shape after unpacked\", x.shape)\n",
    "\n",
    "    with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "        x = ae.decode(x)\n",
    "\n",
    "    x = x.clamp(-1, 1)\n",
    "    x = einops.rearrange(x[0], \"c h w -> h w c\")\n",
    "    img = Image.fromarray((127.5 * (x + 1.0)).cpu().byte().numpy())\n",
    "\n",
    "img\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
